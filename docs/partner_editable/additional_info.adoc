// Add steps as necessary for accessing the software, post-configuration, and testing. Donâ€™t include full usage instructions for your software, but add links to your product documentation for that information.
//Should any sections not be applicable, remove them

== Post deployment steps

To see the capabilities of the QuickStart, follow the below steps to use dataset of meter reads from the City of London, between 2011 to 2014. The <<Customize this Quick Start>> section explains how to edit the raw data ETL script to work with your own meter data.

=== Download Sample Dataset

NOTE: For using the demo dataset, the **LandingzoneTransformer** need to be set to 'london' during the Quick Start deployment.

Download the sample dataset from https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households[here^]. The file can be downloaded to your local machine, unzipped, and then uploaded to S3. However, given the size of the unzipped file (~11GB), it is faster to run a EC2 instance with sufficient privileges to write to S3, download the file to the EC2 instance, unzip it and upload the data from there.

https://data.london.gov.uk/download/smartmeter-energy-use-data-in-london-households/3527bf39-d93e-4071-8451-df2ade1ea4f2/Power-Networks-LCL-June2015(withAcornGps).zip[Download sample meter data^]

=== Upload the dataset to the Raw (Landing zone) S3 bucket
The sample dataset needs to be uploaded to the landing zone bucket. The landing zone is the starting point for AWS Glue workflow. Files that are placed in this S3 bucket will be processed by the ETL pipeline. Furthermore, the AWS Glue ETL workflow tracks which files haven been processed and which are not.

To upload the sample dataset to the Raw S3 bucket, follow the below steps:

. Select the landing zone bucket, buckets names are generated, find the one which contains the word landing zone. This will be the starting point for the ETL process.
+
:xrefstyle: short
[#bucket_layout]
.S3 Bucket layout
image::../images/1_bucket_layout.png[Bucket Layout]

. Upload the London Meter Data to the landing zone bucket.
+
:xrefstyle: short
[#upload_demo_dataset]
.Upload demo data set
image::../images/2_upload_demo_data_set.png[Upload demo data set]

. After the demo data is uploaded, the folder contains one file with all the meter reads.
+
:xrefstyle: short
[#uploaded_demo_dataset]
.Uploaded demo data set
image::../images/3_upload_demo_data_set.png[Uploaded demo data set]

WARNING: Landing Zone S3 bucket should only have meter data files. It shouldn't contain any subfolder, otherwise, the ETL workflow will fail.

//[TODO EC2 command line steps]

=== Prepare weather data if you want to use it for training and forecasting

. https://www.kaggle.com/jeanmidev/smart-meters-in-london?select=weather_hourly_darksky.csv[Download^] the sample weather dataset.
. Upload the dataset to S3 bucket
. Create "weather" table in target Glue database using below SQL statement. Make sure to replace "weather-data-location" to where you upload the weather dataset.
+
:xrefstyle: short
[#create_weather_table]
.Create weather table
image::../images/4_create_weather_table.png[Create weather table via Athena]
+
```sql
CREATE EXTERNAL TABLE weather(
  visibility double,
  windbearing bigint,
  temperature double,
  time string,
  dewpoint double,
  pressure double,
  apparenttemperature double,
  windspeed double,
  preciptype string,
  icon string,
  humidity double,
  summary string )
ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
LOCATION
  's3://<weather-data-location>'
TBLPROPERTIES (
  'has_encrypted_data'='false',
  'skip.header.line.count'='1');
```

If you have other weather dataset you want to use, just make sure it contains at least below 4 columns.

[cols="1,1,1,1", options="header"]
.Weather data schema
|===
|Field
|Type
|Mandatory
|Format
|time |string|X|  yyyy-MM-dd HH:mm:ss
|temperature| double|X|
|apparenttemperature |double|X|
|humidity |double|X|
|===

The CloudFormation parameter "WithWeather" should have been set to 1 to indicate that weather data is available to use, for generating ML model.

=== Prepare geo-location data

This Quick Start requires geo-location data to be uploaded to the _geodata_ folder in Business Zone S3 bucket. You can get the name of the Business zone S3 bucket from the output section of the CloudFormation stack. The S3 path of geo-data location should be _s3://<business-zone-s3-bucket-name>/geodata/_. The data should be in a .CSV file format with the following structure, where all 3 columns are mandatory with comma-separated values.

[cols="1,1", options="header"]
.Geo-location data schema
|===
|Field
|Type
|meter id |string
|latitude |string
|longitude |string
|===

If this data is missing, API request to get meter outage information will return error.


=== Start the AWS Glue ETL and ML training pipeline

By default, the pipeline is configured to be triggered each day at 09:00 AM and process the newly arrived data in the landing zone S3 bucket. However, to process the uploaded data without wait, trigger the pipeline manually. To trigger the ETL pipeline, go to the AWS Glue console and select the 'Workflow' menu entry. The AWS Glue workflow orchestrates the different steps of the ETL process. After the workflow run has successfully completed, Model training state machine is triggered to build the ML model and prepare the data for energy usage forecast and anomaly.

Follow the below steps to start the ETL pipeline.

. Go to the AWS Glue Console, and select 'Workflows'.
+
:xrefstyle: short
[#glue_console]
.AWS Glue Console
image::../images/4_start_etl_workflow.png[AWS Glue Console]

. Select the workflow and choose 'Run' from the drop-down-menu.
+
:xrefstyle: short
[#start_etl_workflow]
.Start ETL workflow
image::../images/5_start_etl_workflow.png[Start ETL workflow]

. The Workflow should be indicated as 'Running' in the 'History' tab.
+
:xrefstyle: short
[#running_workflow]
.Running workflow
image::../images/6_start_etl_workflow.png[Running workflow]

TIP: If the workflow doesn't start and jumps directly to 'Completed' go back to step 2.

== Customize this Quick Start

This Quick Start can be easily customized to use with your own data format. To do that, the first AWS Glue job has to be adjusted to map the incoming data to the internal meter data lake schema. The following steps describe how it can be achieved.

The first AWS Glue Job in the AWS Glue ETL workflow transforms the raw data in the landing zone S3 bucket to cleaned data in the Clean Zone S3 bucket. This step is also responsible for mapping the inbound data to the internal data schema, which is used by the rest of the steps in the AWS ETL workflow and the ML step functions.

To update the data mapping, the AWS Glue job can be edited directly in the web editor.

. Navigate to the AWS Glue Job console.
+
:xrefstyle: short
[#glue_job_console]
.AWS Glue Job console
image::../images/1_edit_etl_job.png[AWS Glue Job console]

. Select the 'transform_raw_to_clean-*' job.
+
:xrefstyle: short
[#edit_etl_job]
.Edit ETL Job
image::../images/2_edit_etl_job.png[Edit ETL Job]

. Open script editor and start editing the input mapping.
+
:xrefstyle: short
[#open_editor]
.Open editor
image::../images/3_open_editor.png[Open editor]

. To adopt a different input format look for the 'ApplyMapping' call and adjust it to your needs  to reflect your custom input format. Currently, the internal model works with the following schema:
+
[cols="1,1,1,1", options="header"]
.Schema
|===
|Field
|Type
|Mandatory
|Format

|meter_id| String| X|
|reading_value| double| X|0.000
|reading_type| String| X|AGG\|INT
|reading_date_time| Timestamp| X|yyyy-MM-dd HH:mm:ss.SSS
|date_str| String|X| yyyyMMdd
|obis_code| String| |
|week_of_year| int| |
|month| int| |
|year| int| |
|hour| int| |
|minute| int| |
|===

include::./ml_pipeline.adoc[]

== Data schema and API i/o format
include::./data_format.adoc[]

== Best practices
* Use Amazon S3 Glacier to archive the meter data from raw, clean and business S3 buckets, for long term storage and cost savings.